{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Creation with Deep Convolutional GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting some hyperparameters\n",
    "batchSize = 64 \n",
    "imageSize = 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amira\\Anaconda3\\envs\\face\\lib\\site-packages\\torchvision\\transforms\\transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  \"please use transforms.Resize instead.\")\n"
     ]
    }
   ],
   "source": [
    "# Creating the transformations\n",
    "transform = transforms.Compose([transforms.Scale(imageSize), transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = dset.CIFAR10(root = './data', download = True, transform = transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size = batchSize, shuffle = True, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the weights_init function that takes as input a neural network m and that will initialize all its weights.\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "G(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (2): ReLU(inplace)\n",
       "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (5): ReLU(inplace)\n",
       "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (8): ReLU(inplace)\n",
       "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (11): ReLU(inplace)\n",
       "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (13): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the generator\n",
    "\n",
    "class G(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(G, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 512, 4, 1, 0, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias = False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "# Creating the generator\n",
    "netG = G()\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (main): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (1): LeakyReLU(0.2, inplace)\n",
       "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (4): LeakyReLU(0.2, inplace)\n",
       "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (7): LeakyReLU(0.2, inplace)\n",
       "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (10): LeakyReLU(0.2, inplace)\n",
       "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (12): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining the discriminator\n",
    "\n",
    "class D(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(D, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias = False),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias = False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace = True),\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias = False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1)\n",
    "\n",
    "# Creating the discriminator\n",
    "netD = D()\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3: Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][0/782] Loss_D: 2.0372 Loss_G: 6.2626\n",
      "[0/1][1/782] Loss_D: 1.2695 Loss_G: 5.8334\n",
      "[0/1][2/782] Loss_D: 1.0389 Loss_G: 5.7944\n",
      "[0/1][3/782] Loss_D: 0.7992 Loss_G: 7.4905\n",
      "[0/1][4/782] Loss_D: 0.7561 Loss_G: 6.6963\n",
      "[0/1][5/782] Loss_D: 1.0341 Loss_G: 7.6299\n",
      "[0/1][6/782] Loss_D: 0.7052 Loss_G: 8.5783\n",
      "[0/1][7/782] Loss_D: 0.5309 Loss_G: 8.5884\n",
      "[0/1][8/782] Loss_D: 0.5063 Loss_G: 8.3508\n",
      "[0/1][9/782] Loss_D: 0.7487 Loss_G: 9.8648\n",
      "[0/1][10/782] Loss_D: 0.5273 Loss_G: 7.5022\n",
      "[0/1][11/782] Loss_D: 1.2085 Loss_G: 12.8490\n",
      "[0/1][12/782] Loss_D: 0.4704 Loss_G: 10.5582\n",
      "[0/1][13/782] Loss_D: 0.2595 Loss_G: 7.1855\n",
      "[0/1][14/782] Loss_D: 2.1340 Loss_G: 14.9745\n",
      "[0/1][15/782] Loss_D: 0.4925 Loss_G: 13.2448\n",
      "[0/1][16/782] Loss_D: 0.3871 Loss_G: 6.3789\n",
      "[0/1][17/782] Loss_D: 4.0433 Loss_G: 14.8669\n",
      "[0/1][18/782] Loss_D: 0.3646 Loss_G: 14.3137\n",
      "[0/1][19/782] Loss_D: 0.3269 Loss_G: 8.0199\n",
      "[0/1][20/782] Loss_D: 2.8710 Loss_G: 15.5737\n",
      "[0/1][21/782] Loss_D: 0.4685 Loss_G: 14.5749\n",
      "[0/1][22/782] Loss_D: 0.2554 Loss_G: 7.8831\n",
      "[0/1][23/782] Loss_D: 3.1442 Loss_G: 15.9800\n",
      "[0/1][24/782] Loss_D: 0.4322 Loss_G: 15.0385\n",
      "[0/1][25/782] Loss_D: 0.3866 Loss_G: 9.0783\n",
      "[0/1][26/782] Loss_D: 1.4004 Loss_G: 15.0984\n",
      "[0/1][27/782] Loss_D: 0.3239 Loss_G: 14.5169\n",
      "[0/1][28/782] Loss_D: 0.1697 Loss_G: 9.0212\n",
      "[0/1][29/782] Loss_D: 1.0275 Loss_G: 15.3181\n",
      "[0/1][30/782] Loss_D: 0.4901 Loss_G: 13.6543\n",
      "[0/1][31/782] Loss_D: 0.2042 Loss_G: 7.5683\n",
      "[0/1][32/782] Loss_D: 2.6377 Loss_G: 18.9684\n",
      "[0/1][33/782] Loss_D: 0.2223 Loss_G: 20.7109\n",
      "[0/1][34/782] Loss_D: 0.2948 Loss_G: 16.5356\n",
      "[0/1][35/782] Loss_D: 0.3485 Loss_G: 8.2159\n",
      "[0/1][36/782] Loss_D: 1.3260 Loss_G: 16.8810\n",
      "[0/1][37/782] Loss_D: 0.1909 Loss_G: 17.9189\n",
      "[0/1][38/782] Loss_D: 0.1445 Loss_G: 13.6820\n",
      "[0/1][39/782] Loss_D: 0.0887 Loss_G: 6.0452\n",
      "[0/1][40/782] Loss_D: 3.2059 Loss_G: 20.8412\n",
      "[0/1][41/782] Loss_D: 0.6158 Loss_G: 23.3938\n",
      "[0/1][42/782] Loss_D: 0.3139 Loss_G: 20.6980\n",
      "[0/1][43/782] Loss_D: 0.2666 Loss_G: 14.1845\n",
      "[0/1][44/782] Loss_D: 0.0746 Loss_G: 6.3344\n",
      "[0/1][45/782] Loss_D: 2.0229 Loss_G: 19.8530\n",
      "[0/1][46/782] Loss_D: 0.2367 Loss_G: 23.0083\n",
      "[0/1][47/782] Loss_D: 0.3747 Loss_G: 20.3920\n",
      "[0/1][48/782] Loss_D: 0.1218 Loss_G: 13.2289\n",
      "[0/1][49/782] Loss_D: 0.1107 Loss_G: 4.8659\n",
      "[0/1][50/782] Loss_D: 3.8688 Loss_G: 20.7935\n",
      "[0/1][51/782] Loss_D: 0.6387 Loss_G: 23.7018\n",
      "[0/1][52/782] Loss_D: 0.5255 Loss_G: 20.9485\n",
      "[0/1][53/782] Loss_D: 0.1837 Loss_G: 14.6291\n",
      "[0/1][54/782] Loss_D: 0.0348 Loss_G: 6.8952\n",
      "[0/1][55/782] Loss_D: 1.2165 Loss_G: 19.3198\n",
      "[0/1][56/782] Loss_D: 0.2767 Loss_G: 21.6568\n",
      "[0/1][57/782] Loss_D: 0.4120 Loss_G: 18.7382\n",
      "[0/1][58/782] Loss_D: 0.1412 Loss_G: 13.3727\n",
      "[0/1][59/782] Loss_D: 0.0572 Loss_G: 6.7927\n",
      "[0/1][60/782] Loss_D: 0.5479 Loss_G: 14.2668\n",
      "[0/1][61/782] Loss_D: 0.1063 Loss_G: 15.0116\n",
      "[0/1][62/782] Loss_D: 0.0707 Loss_G: 11.8288\n",
      "[0/1][63/782] Loss_D: 0.0932 Loss_G: 6.7689\n",
      "[0/1][64/782] Loss_D: 0.2662 Loss_G: 10.0305\n",
      "[0/1][65/782] Loss_D: 0.2457 Loss_G: 8.4662\n",
      "[0/1][66/782] Loss_D: 0.1076 Loss_G: 7.3133\n",
      "[0/1][67/782] Loss_D: 0.4495 Loss_G: 15.1539\n",
      "[0/1][68/782] Loss_D: 0.1047 Loss_G: 15.6954\n",
      "[0/1][69/782] Loss_D: 0.1051 Loss_G: 11.2488\n",
      "[0/1][70/782] Loss_D: 0.3059 Loss_G: 5.4125\n",
      "[0/1][71/782] Loss_D: 2.1079 Loss_G: 25.0781\n",
      "[0/1][72/782] Loss_D: 0.6410 Loss_G: 27.2611\n",
      "[0/1][73/782] Loss_D: 0.4371 Loss_G: 27.2513\n",
      "[0/1][74/782] Loss_D: 0.1683 Loss_G: 27.1415\n",
      "[0/1][75/782] Loss_D: 0.0782 Loss_G: 25.8814\n",
      "[0/1][76/782] Loss_D: 0.0193 Loss_G: 23.3108\n",
      "[0/1][77/782] Loss_D: 0.0264 Loss_G: 17.9661\n",
      "[0/1][78/782] Loss_D: 0.0229 Loss_G: 10.7960\n",
      "[0/1][79/782] Loss_D: 0.0284 Loss_G: 4.9919\n",
      "[0/1][80/782] Loss_D: 1.2180 Loss_G: 20.1378\n",
      "[0/1][81/782] Loss_D: 0.0556 Loss_G: 24.1440\n",
      "[0/1][82/782] Loss_D: 0.2884 Loss_G: 23.1415\n",
      "[0/1][83/782] Loss_D: 0.1682 Loss_G: 19.5504\n",
      "[0/1][84/782] Loss_D: 0.0873 Loss_G: 14.2887\n",
      "[0/1][85/782] Loss_D: 0.1112 Loss_G: 7.9008\n",
      "[0/1][86/782] Loss_D: 0.1308 Loss_G: 6.1176\n",
      "[0/1][87/782] Loss_D: 0.2841 Loss_G: 12.0942\n",
      "[0/1][88/782] Loss_D: 0.0651 Loss_G: 13.0896\n",
      "[0/1][89/782] Loss_D: 0.2284 Loss_G: 10.0834\n",
      "[0/1][90/782] Loss_D: 0.0780 Loss_G: 5.3993\n",
      "[0/1][91/782] Loss_D: 0.3647 Loss_G: 11.0191\n",
      "[0/1][92/782] Loss_D: 0.1057 Loss_G: 10.0642\n",
      "[0/1][93/782] Loss_D: 0.0663 Loss_G: 7.7922\n",
      "[0/1][94/782] Loss_D: 0.0843 Loss_G: 5.4879\n",
      "[0/1][95/782] Loss_D: 0.3885 Loss_G: 11.6162\n",
      "[0/1][96/782] Loss_D: 0.0959 Loss_G: 10.6274\n",
      "[0/1][97/782] Loss_D: 0.3552 Loss_G: 6.1010\n",
      "[0/1][98/782] Loss_D: 0.4446 Loss_G: 10.4691\n",
      "[0/1][99/782] Loss_D: 0.1176 Loss_G: 9.3116\n",
      "[0/1][100/782] Loss_D: 0.1233 Loss_G: 6.4203\n",
      "[0/1][101/782] Loss_D: 0.2353 Loss_G: 7.0277\n",
      "[0/1][102/782] Loss_D: 0.1300 Loss_G: 6.5998\n",
      "[0/1][103/782] Loss_D: 0.2841 Loss_G: 6.9478\n",
      "[0/1][104/782] Loss_D: 0.1215 Loss_G: 6.6198\n",
      "[0/1][105/782] Loss_D: 0.3265 Loss_G: 7.4862\n",
      "[0/1][106/782] Loss_D: 0.2909 Loss_G: 5.2141\n",
      "[0/1][107/782] Loss_D: 0.5475 Loss_G: 9.7202\n",
      "[0/1][108/782] Loss_D: 0.5931 Loss_G: 6.3402\n",
      "[0/1][109/782] Loss_D: 0.2485 Loss_G: 6.4442\n",
      "[0/1][110/782] Loss_D: 0.4315 Loss_G: 8.8423\n",
      "[0/1][111/782] Loss_D: 0.4377 Loss_G: 5.5801\n",
      "[0/1][112/782] Loss_D: 0.4305 Loss_G: 8.8460\n",
      "[0/1][113/782] Loss_D: 0.3640 Loss_G: 5.9254\n",
      "[0/1][114/782] Loss_D: 0.2418 Loss_G: 6.0285\n",
      "[0/1][115/782] Loss_D: 0.2170 Loss_G: 6.4296\n",
      "[0/1][116/782] Loss_D: 0.5060 Loss_G: 1.9448\n",
      "[0/1][117/782] Loss_D: 1.4205 Loss_G: 17.3890\n",
      "[0/1][118/782] Loss_D: 7.6702 Loss_G: 12.4539\n",
      "[0/1][119/782] Loss_D: 2.4530 Loss_G: 4.1704\n",
      "[0/1][120/782] Loss_D: 0.2006 Loss_G: 1.7595\n",
      "[0/1][121/782] Loss_D: 0.7744 Loss_G: 6.0751\n",
      "[0/1][122/782] Loss_D: 0.3168 Loss_G: 6.3060\n",
      "[0/1][123/782] Loss_D: 0.4793 Loss_G: 3.9296\n",
      "[0/1][124/782] Loss_D: 0.2564 Loss_G: 3.6485\n",
      "[0/1][125/782] Loss_D: 0.4569 Loss_G: 5.0277\n",
      "[0/1][126/782] Loss_D: 0.4710 Loss_G: 4.4661\n",
      "[0/1][127/782] Loss_D: 0.6259 Loss_G: 4.5358\n",
      "[0/1][128/782] Loss_D: 0.4056 Loss_G: 5.5300\n",
      "[0/1][129/782] Loss_D: 0.2914 Loss_G: 5.2241\n",
      "[0/1][130/782] Loss_D: 0.4378 Loss_G: 5.6546\n",
      "[0/1][131/782] Loss_D: 0.4287 Loss_G: 3.7626\n",
      "[0/1][132/782] Loss_D: 0.5524 Loss_G: 7.7776\n",
      "[0/1][133/782] Loss_D: 0.6871 Loss_G: 3.6714\n",
      "[0/1][134/782] Loss_D: 0.7337 Loss_G: 5.9184\n",
      "[0/1][135/782] Loss_D: 0.4899 Loss_G: 5.5129\n",
      "[0/1][136/782] Loss_D: 0.3746 Loss_G: 5.0480\n",
      "[0/1][137/782] Loss_D: 0.2218 Loss_G: 4.5070\n",
      "[0/1][138/782] Loss_D: 0.4046 Loss_G: 4.3913\n",
      "[0/1][139/782] Loss_D: 0.3634 Loss_G: 4.4829\n",
      "[0/1][140/782] Loss_D: 0.3250 Loss_G: 4.8222\n",
      "[0/1][141/782] Loss_D: 0.3415 Loss_G: 4.3239\n",
      "[0/1][142/782] Loss_D: 0.4470 Loss_G: 5.7874\n",
      "[0/1][143/782] Loss_D: 0.5360 Loss_G: 2.2462\n",
      "[0/1][144/782] Loss_D: 1.0585 Loss_G: 8.9182\n",
      "[0/1][145/782] Loss_D: 1.5767 Loss_G: 5.0865\n",
      "[0/1][146/782] Loss_D: 0.4821 Loss_G: 5.5150\n",
      "[0/1][147/782] Loss_D: 0.3529 Loss_G: 3.8938\n",
      "[0/1][148/782] Loss_D: 0.3530 Loss_G: 4.6345\n",
      "[0/1][149/782] Loss_D: 0.4779 Loss_G: 4.2366\n",
      "[0/1][150/782] Loss_D: 0.3266 Loss_G: 4.0376\n",
      "[0/1][151/782] Loss_D: 0.5242 Loss_G: 4.2217\n",
      "[0/1][152/782] Loss_D: 0.5225 Loss_G: 2.4156\n",
      "[0/1][153/782] Loss_D: 0.7560 Loss_G: 7.5371\n",
      "[0/1][154/782] Loss_D: 0.7020 Loss_G: 3.3029\n",
      "[0/1][155/782] Loss_D: 0.3242 Loss_G: 4.0138\n",
      "[0/1][156/782] Loss_D: 0.2736 Loss_G: 4.4909\n",
      "[0/1][157/782] Loss_D: 0.3532 Loss_G: 3.8700\n",
      "[0/1][158/782] Loss_D: 0.5785 Loss_G: 5.4235\n",
      "[0/1][159/782] Loss_D: 0.9410 Loss_G: 0.9417\n",
      "[0/1][160/782] Loss_D: 1.2692 Loss_G: 10.7424\n",
      "[0/1][161/782] Loss_D: 2.7766 Loss_G: 4.9475\n",
      "[0/1][162/782] Loss_D: 0.3542 Loss_G: 2.0931\n",
      "[0/1][163/782] Loss_D: 1.1164 Loss_G: 6.8704\n",
      "[0/1][164/782] Loss_D: 1.2843 Loss_G: 2.9320\n",
      "[0/1][165/782] Loss_D: 0.7597 Loss_G: 4.2724\n",
      "[0/1][166/782] Loss_D: 0.3110 Loss_G: 4.1720\n",
      "[0/1][167/782] Loss_D: 0.4504 Loss_G: 3.4763\n",
      "[0/1][168/782] Loss_D: 0.4830 Loss_G: 4.0892\n",
      "[0/1][169/782] Loss_D: 0.3350 Loss_G: 3.4209\n",
      "[0/1][170/782] Loss_D: 0.5701 Loss_G: 4.9913\n",
      "[0/1][171/782] Loss_D: 0.5942 Loss_G: 2.2301\n",
      "[0/1][172/782] Loss_D: 0.5758 Loss_G: 4.7480\n",
      "[0/1][173/782] Loss_D: 0.4725 Loss_G: 3.6984\n",
      "[0/1][174/782] Loss_D: 0.4369 Loss_G: 3.6959\n",
      "[0/1][175/782] Loss_D: 0.4175 Loss_G: 3.9176\n",
      "[0/1][176/782] Loss_D: 0.2713 Loss_G: 4.0585\n",
      "[0/1][177/782] Loss_D: 0.3444 Loss_G: 3.3213\n",
      "[0/1][178/782] Loss_D: 0.4925 Loss_G: 5.3309\n",
      "[0/1][179/782] Loss_D: 0.5594 Loss_G: 2.2232\n",
      "[0/1][180/782] Loss_D: 1.0080 Loss_G: 8.5366\n",
      "[0/1][181/782] Loss_D: 2.1786 Loss_G: 2.9737\n",
      "[0/1][182/782] Loss_D: 0.4939 Loss_G: 4.7728\n",
      "[0/1][183/782] Loss_D: 0.2711 Loss_G: 6.0231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/1][184/782] Loss_D: 0.6554 Loss_G: 2.3739\n",
      "[0/1][185/782] Loss_D: 0.8959 Loss_G: 6.9352\n",
      "[0/1][186/782] Loss_D: 0.4659 Loss_G: 5.0212\n",
      "[0/1][187/782] Loss_D: 0.2869 Loss_G: 4.4527\n",
      "[0/1][188/782] Loss_D: 0.2830 Loss_G: 4.6026\n",
      "[0/1][189/782] Loss_D: 0.3689 Loss_G: 4.3419\n",
      "[0/1][190/782] Loss_D: 0.3553 Loss_G: 4.4878\n",
      "[0/1][191/782] Loss_D: 0.3724 Loss_G: 4.8126\n",
      "[0/1][192/782] Loss_D: 0.3907 Loss_G: 4.1079\n",
      "[0/1][193/782] Loss_D: 0.3878 Loss_G: 5.2398\n",
      "[0/1][194/782] Loss_D: 0.3341 Loss_G: 4.0044\n",
      "[0/1][195/782] Loss_D: 0.3751 Loss_G: 5.4219\n",
      "[0/1][196/782] Loss_D: 0.3974 Loss_G: 4.2566\n",
      "[0/1][197/782] Loss_D: 0.3658 Loss_G: 4.6751\n",
      "[0/1][198/782] Loss_D: 0.3132 Loss_G: 4.8593\n",
      "[0/1][199/782] Loss_D: 0.1535 Loss_G: 5.3713\n",
      "[0/1][200/782] Loss_D: 0.2532 Loss_G: 4.6959\n",
      "[0/1][201/782] Loss_D: 0.2909 Loss_G: 4.3841\n",
      "[0/1][202/782] Loss_D: 0.3778 Loss_G: 5.1628\n",
      "[0/1][203/782] Loss_D: 0.3482 Loss_G: 4.0834\n",
      "[0/1][204/782] Loss_D: 0.3913 Loss_G: 7.1261\n",
      "[0/1][205/782] Loss_D: 0.2935 Loss_G: 4.1379\n",
      "[0/1][206/782] Loss_D: 0.4080 Loss_G: 5.7972\n",
      "[0/1][207/782] Loss_D: 0.1876 Loss_G: 5.5834\n",
      "[0/1][208/782] Loss_D: 0.2465 Loss_G: 4.3979\n",
      "[0/1][209/782] Loss_D: 0.5006 Loss_G: 9.1644\n",
      "[0/1][210/782] Loss_D: 0.9776 Loss_G: 2.4425\n",
      "[0/1][211/782] Loss_D: 1.0007 Loss_G: 13.1855\n",
      "[0/1][212/782] Loss_D: 2.4861 Loss_G: 7.1248\n",
      "[0/1][213/782] Loss_D: 0.2031 Loss_G: 4.7435\n",
      "[0/1][214/782] Loss_D: 0.4547 Loss_G: 8.2489\n",
      "[0/1][215/782] Loss_D: 0.3288 Loss_G: 5.1508\n",
      "[0/1][216/782] Loss_D: 0.3739 Loss_G: 5.8870\n",
      "[0/1][217/782] Loss_D: 0.4001 Loss_G: 6.1910\n",
      "[0/1][218/782] Loss_D: 0.4829 Loss_G: 3.7848\n",
      "[0/1][219/782] Loss_D: 0.6574 Loss_G: 10.5595\n",
      "[0/1][220/782] Loss_D: 0.4489 Loss_G: 8.9059\n",
      "[0/1][221/782] Loss_D: 0.1666 Loss_G: 5.6789\n",
      "[0/1][222/782] Loss_D: 0.1791 Loss_G: 5.4003\n",
      "[0/1][223/782] Loss_D: 0.1754 Loss_G: 6.6788\n",
      "[0/1][224/782] Loss_D: 0.1075 Loss_G: 6.0969\n",
      "[0/1][225/782] Loss_D: 0.0872 Loss_G: 5.3839\n",
      "[0/1][226/782] Loss_D: 0.2499 Loss_G: 6.0138\n",
      "[0/1][227/782] Loss_D: 0.2872 Loss_G: 4.5210\n"
     ]
    }
   ],
   "source": [
    "# Training the DCGANs\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = 0.0002, betas = (0.5, 0.999))\n",
    "\n",
    "for epoch in range(1):\n",
    "\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        # 1st Step: Updating the weights of the neural network of the discriminator\n",
    "\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Training the discriminator with a real image of the dataset\n",
    "        real, _ = data\n",
    "        input = Variable(real)\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(input)\n",
    "        errD_real = criterion(output, target)\n",
    "        \n",
    "        # Training the discriminator with a fake image generated by the generator\n",
    "        noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n",
    "        fake = netG(noise)\n",
    "        target = Variable(torch.zeros(input.size()[0]))\n",
    "        output = netD(fake.detach())\n",
    "        errD_fake = criterion(output, target)\n",
    "        \n",
    "        # Backpropagating the total error\n",
    "        errD = errD_real + errD_fake\n",
    "        errD.backward()\n",
    "        optimizerD.step()\n",
    "\n",
    "        # 2nd Step: Updating the weights of the neural network of the generator\n",
    "\n",
    "        netG.zero_grad()\n",
    "        target = Variable(torch.ones(input.size()[0]))\n",
    "        output = netD(fake)\n",
    "        errG = criterion(output, target)\n",
    "        errG.backward()\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n",
    "\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f' % (epoch, 1, i, len(dataloader), errD.data[0], errG.data[0]))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize = True)\n",
    "            fake = netG(noise)\n",
    "            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
